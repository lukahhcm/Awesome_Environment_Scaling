# üèñÔ∏è Awesome-Environment-Scaling

<div align="center">
  <h1 align="center">Scaling Environments: A Crucial Step towards Agent Intelligence</h1>

  <p align="center">
    This repository accompanies our survey paper: <br>
    <a href=""><strong>Scaling Environments for LLM Agents in the Era of
Learning from Interaction: A Survey</strong></a>
  </p>
</div>


<div align="center">
  <a href="">
    <img src="https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white" alt="Paper">
  </a>
  <a href="https://github.com/lukahhcm/Awesome_Environment_Scaling">
    <img src="https://img.shields.io/badge/Environment_Scaling-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white" alt="Github">
  </a>
  <a href="">
    <img src="https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000" alt="Hugging Face Collection">
  </a>
  <!-- <a href="https://x.com/SuZhaochen0110/status/1940251163166986333">
    <img src="https://img.shields.io/badge/Twitter-%23000000.svg?style=for-the-badge&logo=twitter&logoColor=white" alt="Twitter">
  </a> -->
</div>

## üéâ Introduction

<div align="center">
  <img src="image/teaser.jpg" alt="(A) Experience arises from the Generation‚ÄìExecution‚ÄìFeedback (GEF) loop, where environments generate tasks, agents execute them, and environments evaluate and filter useful experience for RL training. (B) Overview of environment scaling: a GEF-aligned taxonomy of environment-scaling methods, alongside implementation, applications, and the unique challenge of Generator‚ÄìVerifier asymmetry." width="800">
</div>

Welcome to **Awesome-Scaling-Environments!**

As agent capabilities continue to evolve, it is infeasible to attain intelligence beyond the human-level merely by supervised fine-tuning (SFT) pretrained models on static datasets. Such datasets are typically manually annotated or curated under human oversight, which makes them costly and labor-intensive to produce at scale, intrinsically bounded by human-level knowledge, and often lacking realism and adaptability. By contrast, *reinforcement learning* provides a more aligned training paradigm: agents explore and interact with the environment, accumulate experience, and acquire new knowledge and skills. We formalize this interactive process as *the Generation‚ÄìExecution‚ÄìFeedback (GEF) loop*. In this setting, the environment is no longer a mere container for agents‚Äô activities; it has become an active producer of experiential data, underscoring the growing need for scaling environments to create a more **complex**, **realistic**, and **richly interactive** world.

We are the first to comprehensively investigate current environment scaling methods from an **environment-centric** perspective and curate representative methods along the three stages of the GEF loop:

- **Stage 1: Task Generation** - Environments generate *complex*, *dynamic*, and *diverse* tasks to continuously challenge agents.

- **Stage 2: Task Execution** - Environments support real-time interaction *(Interactivity)* and provide observation that consitent with real world *(Realism)*.

- **Stage 3: Feedback** - Environments provide evaluative feedback on rollouts with *higher density* and *finer granularity*, increase *automation* and *objectivity*, and improve *robustness*, forming useful experience for subsequent learning.


<div align="center">
  <img src="image/task.jpg" alt="" width="800">
  <em>Environment scaling in the task generation and task execution stages.</em>
</div>
<div align="center">
  <img src="image/feedback.jpg" alt="" width="800">
  <em>Environment scaling in the feedback stage.</em>
</div>

This collection is for researchers, developers, and enthusiasts eager to explore and build the next generation of LLM agents.
<div align="center">
  <img src="image/taxonomy.png" alt="" width="800">
  <em>GEF-aligned taxonomy of environment scaling.</em>
</div>

<!-- ## üîî News
-   **[2025-07]** We have released ["Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers"](https://arxiv.org/pdf/2506.23918), the **first comprehensive survey** dedicated to the emerging paradigm of "Think with Images". 
-   **[2025-06]** We created this repository to maintain a paper list on Awesome-Think-With-Images. Contributions are welcome!
-   **[2025-05]** We are excited to release **[OpenThinkIMG](https://github.com/OpenThinkIMG/OpenThinkIMG)**, the first dedicated end-to-end open-source framework designed to empower LVLMs to truly **think with images**! For ease of use, we've configured a Docker environment. We warmly invite the community to explore, use, and contribute. -->

## üìú Table of Contents
- [üèñÔ∏è Awesome-Environment-Scaling](#Ô∏è-awesome-environment-scaling)
  - [üéâ Introduction](#-introduction)
  - [üìú Table of Contents](#-table-of-contents)
  - [üéØ Stage 1: Task Generation](#-stage-1-task-generation)
    - [‚û§ Complexity Scaling](#-complexity-scaling)
    - [‚û§ Dynamic Scaling](#-dynamic-scaling)
    - [‚û§ Diversity Scaling](#-diversity-scaling)
  - [üßó Stage 2: Task Execution](#-stage-2-task-execution)
    - [‚û§ Interactivity Scaling](#-interactivity-scaling)
    - [‚û§ Realism Scaling](#-realism-scaling)
  - [‚öñÔ∏è Stage 3: Feedback](#Ô∏è-stage-3-feedback)
    - [‚û§ Density Scaling](#-density-scaling)
    - [‚û§ Granularity Scaling](#-granularity-scaling)
    - [‚û§ Automation Scaling](#-automation-scaling)
    - [‚û§ Objectivity Scaling](#-objectivity-scaling)
    - [‚û§ Robustness Scaling](#-robustness-scaling)
  - [üôè Contributing \& Citation](#-contributing--citation)
  - [Star History](#star-history)


## üéØ Stage 1: Task Generation
In the task generation stage, the environment is required to propose challenging tasks to push the agent‚Äôs capabilities forward. Scaling at this stage therefore targets three facets of task supply: increasing difficulty, introducing dynamics, and expanding diversity. Accordingly, we organize representative approaches into three directions: *complexity scaling*, *dynamic scaling*, and *diversity scaling*.

### ‚û§ Complexity Scaling
*Static complexity increases a task‚Äôs inherent structural intricacy, moving beyond single-step commands to challenges defined by dependencies, logical flows, and hierarchical relationships.*

Sequential
- [$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking](https://arxiv.org/abs/2505.18746v1) ![](https://img.shields.io/badge/abs-2025.05-red)
- [$\tau$-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains](https://arxiv.org/abs/2406.12045) ![](https://img.shields.io/badge/abs-2024.06-red)
- [The Berkeley Function Calling Leaderboard (BFCL): From Tool Use to Agentic Evaluation of Large Language Models](https://openreview.net/forum?id=2GmDdhBdDk)
- [ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs](https://arxiv.org/abs/2307.16789) ![](https://img.shields.io/badge/abs-2023.07-red)

Compositional
- [Taskcraft: Automated generation of agentic tasks.](https://arxiv.org/abs/2506.10055) ![](https://img.shields.io/badge/abs-2025.06-red)
- [Compositional Task Representations for Large Language Models](https://openreview.net/forum?id=6axIMJA7ME3)

Graph-based
- [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592) ![](https://img.shields.io/badge/abs-2025.07-red)
- [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/abs/2507.15061) ![](https://img.shields.io/badge/abs-2025.07-red)
- [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/abs/2505.22648) ![](https://img.shields.io/badge/abs-2025.05-red)
- [WebWalker: Benchmarking LLMs in Web Traversal](https://arxiv.org/abs/2501.07572) ![](https://img.shields.io/badge/abs-2025.01-red)

### ‚û§ Dynamic Scaling
*Scaling dynamics establishes non-stationary learning targets and distinct action and state spaces, which is critical for encouraging robust agent generalization*

Task Difficulty Dynamics
- [Eurekaverse: Environment Curriculum Generation via Large Language Models](https://arxiv.org/abs/2411.01775) ![](https://img.shields.io/badge/abs-2024.11-red)
- [WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning](https://arxiv.org/abs/2411.02337) ![](https://img.shields.io/badge/abs-2024.11-red)
- [AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation](https://arxiv.org/abs/2408.00764) ![](https://img.shields.io/badge/abs-2024.08-red) 
- [AgentGym: Evolving Large Language Model-based Agents across Diverse Environments](https://arxiv.org/abs/2406.04151) ![](https://img.shields.io/badge/abs-2024.06-red) 
- [EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making](https://arxiv.org/abs/2508.09586) ![](https://img.shields.io/badge/abs-2025.08-red)
- [R-Zero: Self-Evolving Reasoning LLM from Zero Data](https://arxiv.org/abs/2508.05004) ![](https://img.shields.io/badge/abs-2025.08-red)

Environmental Dynamics
- [ARE: Scaling Up Agent Environments and Evaluations](https://arxiv.org/abs/2509.17158) ![](https://img.shields.io/badge/abs-2025.09-red)
- [EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents](https://arxiv.org/abs/2403.12014) ![](https://img.shields.io/badge/abs-2024.03-red)

### ‚û§ Diversity Scaling
*Scaling the diversity of tasks and environments is essential for developing robust and generalizable LLM agents.*
- [R-Zero: Self-Evolving Reasoning LLM from Zero Data](https://arxiv.org/abs/2508.05004) ![](https://img.shields.io/badge/abs-2025.08-red)
- [AgentSense: Virtual Sensor Data Generation Using LLM Agents in Simulated Home Environments](https://arxiv.org/abs/2506.11773) ![](https://img.shields.io/badge/abs-2025.06-red)
- [AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories](https://arxiv.org/abs/2410.07706) ![](https://img.shields.io/badge/abs-2024.10-red)
- [AgentGym: Evolving Large Language Model-based Agents across Diverse Environments](https://arxiv.org/abs/2406.04151) ![](https://img.shields.io/badge/abs-2024.06-red) 

## üßó Stage 2: Task Execution
In the task execution stage, after the agent takes an action, it receives an observation from the environment. Consequently, whether the agent can interact with the environment in real time (interactivity) and whether the returned observations are consistent with real-world scenarios (realism) are both critical to the quality of the resulting experience. 
### ‚û§ Interactivity Scaling
- [Towards General Agentic Intelligence via Environment Scaling](https://arxiv.org/abs/2509.13311) ![](https://img.shields.io/badge/abs-2025.09-red)
- [BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair](https://arxiv.org/abs/2508.09129) ![](https://img.shields.io/badge/abs-2025.08-red)
- [MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers](https://arxiv.org/abs/2508.14704) ![](https://img.shields.io/badge/abs-2025.08-red)
- [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers](https://www.arxiv.org/abs/2508.20453) ![](https://img.shields.io/badge/abs-2025.08-red)
- [MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark](https://arxiv.org/abs/2508.07575) ![](https://img.shields.io/badge/abs-2025.08-red)
- [Procedural Environment Generation for Tool-Use Agents](https://arxiv.org/abs/2506.11045) ![](https://img.shields.io/badge/abs-2025.06-red)
- [AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents](https://arxiv.org/abs/2407.18901) ![](https://img.shields.io/badge/abs-2024.07-red)
- [WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents](https://arxiv.org/abs/2207.01206) ![](https://img.shields.io/badge/abs-2022.07-red)

### ‚û§ Realism Scaling
- [ARE: Scaling Up Agent Environments and Evaluations](https://arxiv.org/abs/2509.17158) ![](https://img.shields.io/badge/abs-2025.09-red)
- [Towards General Agentic Intelligence via Environment Scaling](https://arxiv.org/abs/2509.13311) ![](https://img.shields.io/badge/abs-2025.09-red)
- [Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments](https://arxiv.org/abs/2508.08791) ![](https://img.shields.io/badge/abs-2025.08-red)
- [Making REST APIs Agent-Ready: From OpenAPI to Model Context Protocol Servers for Tool-Augmented LLMs](https://arxiv.org/abs/2507.16044v2) ![](https://img.shields.io/badge/abs-2025.07-red)
- [$\tau^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment](https://arxiv.org/abs/2506.07982) ![](https://img.shields.io/badge/abs-2025.06-red)
- [APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay](https://arxiv.org/abs/2504.03601) ![](https://img.shields.io/badge/abs-2025.04-red)
- [WebWalker: Benchmarking LLMs in Web Traversal](https://arxiv.org/abs/2501.07572) ![](https://img.shields.io/badge/abs-2025.02-red)
- [AgentSociety: Large-Scale Simulation of LLM-Driven Generative Agents Advances Understanding of Human Behaviors and Society](https://arxiv.org/abs/2502.08691) ![](https://img.shields.io/badge/abs-2025.02-red)
- [ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use](https://arxiv.org/abs/2501.02506) ![](https://img.shields.io/badge/abs-2025.01-red)
- [OASIS: Open Agent Social Interaction Simulations with One Million Agents](https://arxiv.org/abs/2411.11581) ![](https://img.shields.io/badge/abs-2024.11-red)
- [$\tau$-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains](https://arxiv.org/abs/2406.12045) ![](https://img.shields.io/badge/abs-2024.06-red)
- [RestGPT: Connecting Large Language Models with Real-World RESTful APIs](https://arxiv.org/abs/2306.06624) ![](https://img.shields.io/badge/abs-2023.06-red)
- [Tongyi DeepResearch: A New Era of Open-Source AI Researchers](https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/)


## ‚öñÔ∏è Stage 3: Feedback
In the feedback stage, the environment assesses the trajectories collected during task execution and generates feedback signals for subsequent RL training. Scaling at this stage focuses on how feedback is provided, including its frequency and richness (density and granularity), its level of automation (automation), as well as how objectively and reliably it is delivered (objectivity and robustness).

### ‚û§ Density Scaling
Outcome-based Rewards 
- [VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use](https://arxiv.org/abs/2509.01055) ![](https://img.shields.io/badge/abs-2025.09-red)
- [Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards](https://arxiv.org/abs/2506.11425) ![](https://img.shields.io/badge/abs-2025.06-red)
- [Toolrl: Reward is all tool learning needs](https://arxiv.org/abs/2504.13958) ![](https://img.shields.io/badge/abs-2025.04-red)
- [OTC: Optimal Tool Calls via Reinforcement Learning](https://arxiv.org/abs/2504.14870v1) ![](https://img.shields.io/badge/abs-2025.04-red)


Process-based Rewards 
- [Know What You Don't Know: Uncertainty Calibration of Process Reward Models](https://arxiv.org/abs/2506.09338) ![](https://img.shields.io/badge/abs-2025.06-red)
- [Web-Shepherd: Advancing PRMs for Reinforcing Web Agents](https://arxiv.org/abs/2505.15277) ![](https://img.shields.io/badge/abs-2025.05-red)
- [Process Reward Models That Think](https://arxiv.org/abs/2504.16828) ![](https://img.shields.io/badge/abs-2025.04-red)
- [What Are Step-Level Reward Models Rewarding? Counterintuitive Findings from MCTS-Boosted Mathematical Reasoning](https://arxiv.org/abs/2412.15904) ![](https://img.shields.io/badge/abs-2024.12-red)
- [On Designing Effective RL Reward at Training Time for LLM Reasoning](https://arxiv.org/abs/2410.15115) ![](https://img.shields.io/badge/abs-2024.10-red)

### ‚û§ Granularity Scaling
- [Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training](https://arxiv.org/abs/2509.21500) ![](https://img.shields.io/badge/abs-2025.09-red)
- [Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning](https://arxiv.org/abs/2508.16949) ![](https://img.shields.io/badge/abs-2025.08-red)
- [Reinforcement Learning with Rubric Anchors](https://arxiv.org/abs/2508.12790) ![](https://img.shields.io/badge/abs-2025.08-red)
- [Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains](https://arxiv.org/abs/2507.17746) ![](https://img.shields.io/badge/abs-2025.07-red)
- [Checklists Are Better Than Reward Models For Aligning Language Models](https://arxiv.org/abs/2507.18624) ![](https://img.shields.io/badge/abs-2025.07-red)
- [AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting](https://arxiv.org/abs/2505.18822) ![](https://img.shields.io/badge/abs-2025.05-red)
- [Toolrl: Reward is all tool learning needs](https://arxiv.org/abs/2504.13958) ![](https://img.shields.io/badge/abs-2025.04-red)
- [R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2503.05592) ![](https://img.shields.io/badge/abs-2025.03-red)

### ‚û§ Automation Scaling
- [JudgeLRM: Large Reasoning Models as a Judge](https://arxiv.org/abs/2504.00050) ![](https://img.shields.io/badge/abs-2025.04-red)
- [Agentic reward modeling: Integrating human preferences with verifiable correctness signals for reliable reward systems](https://arxiv.org/abs/2502.19328) ![](https://img.shields.io/badge/abs-2025.02-red)
- [Scaling autonomous agents via automatic reward modeling and planning](https://arxiv.org/abs/2502.12130) ![](https://img.shields.io/badge/abs-2025.02-red)
- [Generative verifiers: Reward modeling as next-token prediction](https://arxiv.org/abs/2408.15240) ![](https://img.shields.io/badge/abs-2024.08-red)


### ‚û§ Objectivity Scaling
- [ARE: Scaling Up Agent Environments and Evaluations](https://arxiv.org/abs/2509.17158) ![](https://img.shields.io/badge/abs-2025.09-red)
- [Omni-think: Scaling cross-domain generalization in llms via multi-task rl with hybrid rewards](https://arxiv.org/abs/2507.14783) ![](https://img.shields.io/badge/abs-2025.07-red)
- [Rlpr: Extrapolating rlvr to general domains without verifiers](https://arxiv.org/abs/2506.18254) ![](https://img.shields.io/badge/abs-2025.06-red)
- [Writing-zero: Bridge the gap between non-verifiable tasks and verifiable rewards](https://arxiv.org/abs/2506.00103) ![](https://img.shields.io/badge/abs-2025.06-red)
- [General-reasoner: Advancing llm reasoning across all domains](https://arxiv.org/abs/2505.14652) ![](https://img.shields.io/badge/abs-2025.05-red)
- [Nover: Incentive training for language models via verifier-free reinforcement learning](https://arxiv.org/abs/2505.16022) ![](https://img.shields.io/badge/abs-2025.05-red)
- [Nemotron-crossthink: Scaling self-learning beyond math reasoning](https://arxiv.org/abs/2504.13941) ![](https://img.shields.io/badge/abs-2025.04-red)
- [Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains](https://arxiv.org/abs/2503.23829) ![](https://img.shields.io/badge/abs-2025.03-red)


### ‚û§ Robustness Scaling
Reward-level
- [Reward Hacking Mitigation using Verifiable Composite Rewards](https://arxiv.org/abs/2509.15557) ![](https://img.shields.io/badge/abs-2025.09-red)
- [Reward Shaping to Mitigate Reward Hacking in RLHF](https://arxiv.org/abs/2502.18770) ![](https://img.shields.io/badge/abs-2025.02-red)
- [Mona: Myopic optimization with non-myopic approval can mitigate multi-step reward hacking](https://arxiv.org/abs/2501.13011) ![](https://img.shields.io/badge/abs-2025.01-red)
- [Navigating noisy feedback: Enhancing reinforcement learning with error-prone language models](https://arxiv.org/abs/2410.17389) ![](https://img.shields.io/badge/abs-2024.10-red)
- [RRM: Robust Reward Model Training Mitigates Reward Hacking](https://arxiv.org/abs/2409.13156) ![](https://img.shields.io/badge/abs-2024.09-red)
- [InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling](https://arxiv.org/abs/2402.09345) ![](https://img.shields.io/badge/abs-2024.02-red)

Environment-level
- [Trinity-rft: A general-purpose and unified framework for reinforcement fine-tuning of large language models](https://arxiv.org/abs/2505.17826) ![](https://img.shields.io/badge/abs-2025.05-red)
- [Tongyi DeepResearch: A New Era of Open-Source AI Researchers](https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/)
<!-- ## üìä Evaluation & Benchmarks -->

## üôè Contributing & Citation

We welcome contributions! If you have a paper that fits into this framework, please open a pull request. Let's build this resource together. 

If you find our survey and this repository useful for your research, please consider citing our work:

```bibtex

```

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=lukahhcm/Awesome_Scaling_Environments&type=Date)](https://www.star-history.com/#lukahhcm/Awesome_Scaling_Environments&Date)